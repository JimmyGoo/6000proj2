{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F4_EDszaslX",
        "outputId": "a106c741-9178-41b2-bccb-0f8fdca0b679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# to use the packages in google drive\n",
        "sys.path.append('/content/drive/My Drive/6000M_proj2/proj2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "lfqJBa76cX-P"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru"
      ],
      "metadata": {
        "id": "AZMsm6Jkgfgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b345d6e4-d520-49be-a233-ba325172dd50"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.9/dist-packages (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, LeakyReLU, LSTM, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from scipy.stats import skew, kurtosis\n",
        "from os.path import join\n",
        "\n",
        "from config import *\n",
        "from src.universe import Universe\n",
        "from src.utils import time_series_generator\n",
        "from src.metrics import plot_mse"
      ],
      "metadata": {
        "id": "Nm5qK_NGcQ34"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "id": "YYQ_E_F0fkRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee525a7-bd39-41c4-8461-b23fa8d1175a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.9/dist-packages (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.experimental.output_all_intermediates(True)\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, LeakyReLU, LSTM, BatchNormalization\n",
        "from keras.layers import Dense, Input, LSTM, Layer, TimeDistributed, Lambda, Activation, Add\n",
        "from keras.layers import GlobalMaxPooling1D, Flatten, GlobalAveragePooling1D, Reshape, concatenate\n",
        "from tensorflow.keras.initializers import Ones, Zeros\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()\n",
        "\n",
        "# refer to # # https://www.kaggle.com/shujian/transformer-with-lstm\n",
        "# define the multi-head transformer structure by hand\n",
        "class LayerNormalization(Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self._eps = eps\n",
        "        super(LayerNormalization, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self._gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                      initializer=Ones(), trainable=True)\n",
        "        self._beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                     initializer=Zeros(), trainable=True)\n",
        "        super(LayerNormalization, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        mean = K.mean(x, axis=-1, keepdims=True)\n",
        "        std = K.std(x, axis=-1, keepdims=True)\n",
        "        return self._gamma * (x - mean) / (std + self._eps) + self._beta\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention():\n",
        "    def __init__(self, d_model, attn_dropout=0.1):\n",
        "        self._temper = np.sqrt(d_model)\n",
        "        self._dropout = Dropout(attn_dropout)\n",
        "\n",
        "    def __call__(self, q, k, v, mask):\n",
        "        attn = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[2, 2]) / self._temper)([q, k])\n",
        "        if mask is not None:\n",
        "            mmask = Lambda(lambda x: (-1e+10) * (1 - x))(mask)\n",
        "            attn = Add()([attn, mmask])\n",
        "        attn = Activation('softmax')(attn)\n",
        "        attn = self._dropout(attn)\n",
        "        output = Lambda(lambda x: K.batch_dot(x[0], x[1]))([attn, v])\n",
        "        return output, attn\n",
        "\n",
        "\n",
        "class MultiHeadAttention():\n",
        "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
        "    def __init__(self, num_head, dim_model, dim_key, dim_value, dropout, mode=0, use_norm=True):\n",
        "        self._head_mode = mode\n",
        "        self._num_head = num_head\n",
        "        self._dim_key = dim_key\n",
        "        self._dim_value = dim_value\n",
        "        self._dropout = dropout\n",
        "        if self._head_mode == 0:\n",
        "            self.qs_layer = Dense(num_head * dim_key, use_bias=False)\n",
        "            self.ks_layer = Dense(num_head * dim_key, use_bias=False)\n",
        "            self.vs_layer = Dense(num_head * dim_value, use_bias=False)\n",
        "        elif self._head_mode == 1:\n",
        "            self.qs_layers = []\n",
        "            self.ks_layers = []\n",
        "            self.vs_layers = []\n",
        "            for _ in range(num_head):\n",
        "                self.qs_layers.append(TimeDistributed(Dense(dim_key, use_bias=False)))\n",
        "                self.ks_layers.append(TimeDistributed(Dense(dim_key, use_bias=False)))\n",
        "                self.vs_layers.append(TimeDistributed(Dense(dim_value, use_bias=False)))\n",
        "        self.attention = ScaledDotProductAttention(dim_model)\n",
        "        self.layer_norm = LayerNormalization() if use_norm else None\n",
        "        self.w_o = TimeDistributed(Dense(dim_model))\n",
        "\n",
        "    def __call__(self, q, k, v, mask=None):\n",
        "        from keras.layers import Concatenate\n",
        "        if self._head_mode == 0:\n",
        "            qs = self.qs_layer(q)\n",
        "            ks = self.ks_layer(k)\n",
        "            vs = self.vs_layer(v)\n",
        "\n",
        "            def reshape1(x):\n",
        "                shape = tf.shape(x)\n",
        "                x = tf.reshape(x, [shape[0], shape[1], self._num_head, self._dim_key])\n",
        "                x = tf.transpose(x, [2, 0, 1, 3])\n",
        "                x = tf.reshape(x, [-1, shape[1], self._dim_key])\n",
        "                return x\n",
        "\n",
        "            qs = Lambda(reshape1)(qs)\n",
        "            ks = Lambda(reshape1)(ks)\n",
        "            vs = Lambda(reshape1)(vs)\n",
        "\n",
        "            if mask is not None:\n",
        "                mask = Lambda(lambda x: K.repeat_elements(x, self._num_head, 0))(mask)\n",
        "            head, attn = self.attention(qs, ks, vs, mask=mask)\n",
        "\n",
        "            def reshape2(x):\n",
        "                shape = tf.shape(x)\n",
        "                x = tf.reshape(x, [self._num_head, -1, shape[1], shape[2]])\n",
        "                x = tf.transpose(x, [1, 2, 0, 3])\n",
        "                x = tf.reshape(x, [-1, shape[1], self._num_head * self._dim_value])\n",
        "                return x\n",
        "\n",
        "            head = Lambda(reshape2)(head)\n",
        "        elif self._head_mode == 1:\n",
        "            heads = [];\n",
        "            attns = []\n",
        "            for i in range(self._num_head):\n",
        "                qs = self.qs_layers[i](q)\n",
        "                ks = self.ks_layers[i](k)\n",
        "                vs = self.vs_layers[i](v)\n",
        "                head, attn = self.attention(qs, ks, vs, mask)\n",
        "                heads.append(head);\n",
        "                attns.append(attn)\n",
        "            head = Concatenate()(heads) if self._num_head > 1 else heads[0]\n",
        "            attn = Concatenate()(attns) if self._num_head > 1 else attns[0]\n",
        "\n",
        "        outputs = self.w_o(head)\n",
        "        outputs = Dropout(self._dropout)(outputs)\n",
        "        if not self.layer_norm: return outputs, attn\n",
        "        # outputs = Add()([outputs, q]) # sl: fix\n",
        "        return self.layer_norm(outputs), attn"
      ],
      "metadata": {
        "id": "KzGbw75ms6P8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer:\n",
        "    def __init__(self, win_len, input_dim, hidden_dim=[128, 64], attn_mode=0, head_mode=0, scd_layer=True):\n",
        "        self.name = 'Transformer'\n",
        "        self._win_len = win_len\n",
        "        self._input_dim = input_dim\n",
        "        self._hidden_dim = hidden_dim\n",
        "        # if attn_mode = 1, it will deploy the handmade multihead attention layer\n",
        "        # if attn_mode = 0, it will deploy the keras multihead attention layer\n",
        "        self._attn_mode = attn_mode\n",
        "        self._head_mode = head_mode\n",
        "        self._scd_layer = scd_layer\n",
        "\n",
        "    def build_and_compile(self) -> None:\n",
        "        inputs = Input(shape=(self._win_len, self._input_dim))\n",
        "        x = LSTM(self._hidden_dim[0], return_sequences=True)(inputs)\n",
        "        if self._scd_layer == True:\n",
        "          x = LSTM(self._hidden_dim[1], return_sequences=True)(x)\n",
        "        if self._attn_mode == 1:\n",
        "            x, slf_attn = MultiHeadAttention(num_head=3, dim_model=300,\n",
        "                                             dim_key=self._hidden_dim[1],\n",
        "                                             dim_value=self._hidden_dim[1],\n",
        "                                             dropout=0.1,\n",
        "                                             mode=self._head_mode)(x, x, x)\n",
        "        elif self._attn_mode == 0:\n",
        "            x, slf_attn = layers.MultiHeadAttention(\n",
        "                num_heads=3,\n",
        "                key_dim=self._hidden_dim[1],\n",
        "                value_dim=self._hidden_dim[1],\n",
        "                dropout=0.1)(x, x, x,\n",
        "                             return_attention_scores=True)\n",
        "        else:\n",
        "            print('Wrong attn_mode input. Attention layer will not be deployed.')\n",
        "        avg_pool = GlobalAveragePooling1D()(x)\n",
        "        max_pool = GlobalMaxPooling1D()(x)\n",
        "        conc = concatenate([avg_pool, max_pool])\n",
        "        dense = Dense(self._hidden_dim[1], activation=\"relu\")(conc)\n",
        "        dense_out = Dense(self._input_dim)(dense)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=dense_out)\n",
        "\n",
        "        model.compile(\n",
        "            loss='mse',\n",
        "            metrics=['mse'],\n",
        "        )\n",
        "\n",
        "        self._model = model\n",
        "\n",
        "    def get_model(self) -> Model:\n",
        "        return self._model\n",
        "\n",
        "    def summary(self) -> None:\n",
        "        self._model.summary()\n",
        "\n",
        "    def fit(self, **kwargs) -> None:\n",
        "        self._model.fit(**kwargs)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return self._model.predict(X_test)\n",
        "        "
      ],
      "metadata": {
        "id": "0fS3lsTbsk64"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner\n",
        "\n",
        "def build_model(hp):\n",
        "    hid1 = hp.Int(\"hidden1\", min_value=32, max_value=512, step=32)\n",
        "    hid2 = hp.Int(\"hidden2\", min_value=32, max_value=512, step=32)\n",
        "    attn_mode = hp.Int(\"attn\", min_value=0, max_value=1, step=1)\n",
        "    head_mode = hp.Int(\"head\", min_value=0, max_value=1, step=1)\n",
        "    scd_layer = hp.Boolean(\"Second Layer\")\n",
        "    hidden_dim = [hid1, hid2]\n",
        "    build_transformer = Transformer(30, 2500, hidden_dim=hidden_dim, attn_mode=attn_mode, head_mode=head_mode, scd_layer=scd_layer)\n",
        "    build_transformer.build_and_compile()\n",
        "    model = build_transformer.get_model()\n",
        "    return model\n",
        "\n",
        "\n",
        "build_model(keras_tuner.HyperParameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI5nzISfbX9t",
        "outputId": "3222c5b6-7a3f-4807-f37d-9e9122593ebe"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7f923fed19d0>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_loss\",\n",
        "    max_trials=10,\n",
        "    executions_per_trial=10,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"Transformer\",\n",
        ")"
      ],
      "metadata": {
        "id": "1buGrzE3cNvT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWBw0zCkcUSE",
        "outputId": "0d786695-67a7-4e6b-eefe-21438c694e80"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 5\n",
            "hidden1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "hidden2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "attn (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 0, 'max_value': 1, 'step': 1, 'sampling': 'linear'}\n",
            "head (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 0, 'max_value': 1, 'step': 1, 'sampling': 'linear'}\n",
            "Second Layer (Boolean)\n",
            "{'default': False, 'conditions': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inception_date = '2022-03-31'\n",
        "ONE_YEAR_TRADE_DAYS = 252\n",
        "TWO_YEAR_TRADE_DAYS = ONE_YEAR_TRADE_DAYS * 2\n",
        "WIN_LEN = 30\n",
        "UNIVERSE_SIZE = 2500\n",
        "EPOCH = 20\n",
        "BATCH_SIZE = 16\n",
        "training_path = Path(join(data_path, 'train_set'))\n",
        "ret_train = pd.read_csv(join(training_path, '2022-03-31.csv'), index_col=0)\n",
        "X, y = time_series_generator(ret_train, WIN_LEN)\n",
        "train_index = ret_train.loc[:inception_date].iloc[-TWO_YEAR_TRADE_DAYS:].index\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y)\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENXsQr7MfrxS",
        "outputId": "2a2f70f1-7d7d-4ae5-bd55-8872078eb9c8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((378, 30, 2500), (126, 30, 2500), (378, 2500), (126, 2500))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(X_train, y_train, epochs=2, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9Yhn6xvcUvN",
        "outputId": "c606e1cb-5be7-45d2-ecbf-04e91f51d4c7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 02m 13s]\n",
            "val_loss: 0.009882439622685077\n",
            "\n",
            "Best val_loss So Far: 0.002595622382969374\n",
            "Total elapsed time: 00h 37m 56s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]"
      ],
      "metadata": {
        "id": "khtWGmHwpjIu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.build(input_shape=(None, 30, 2500))"
      ],
      "metadata": {
        "id": "Bh_97J_Qppev"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96j5M9oLpzPL",
        "outputId": "38e5c613-745e-4b8d-ae45-d97ef2a3340b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 30, 2500)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 30, 480)      5723520     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  ((None, 30, 480),   738912      ['lstm[0][0]',                   \n",
            " dAttention)                     (None, 3, 30, 30))               'lstm[0][0]',                   \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 480)         0           ['multi_head_attention[0][0]']   \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 480)         0           ['multi_head_attention[0][0]']   \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 960)          0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 , 'global_max_pooling1d[0][0]']  \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          123008      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2500)         322500      ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,907,940\n",
            "Trainable params: 6,907,940\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTzsiHfvp5eH",
        "outputId": "4579e43f-1811-4ff4-9d68-5b931863aa4a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in my_dir/Transformer\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_loss\", direction=\"min\")\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "hidden1: 480\n",
            "hidden2: 128\n",
            "attn: 0\n",
            "head: 0\n",
            "Second Layer: False\n",
            "Score: 0.002595622382969374\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "hidden1: 64\n",
            "hidden2: 64\n",
            "attn: 0\n",
            "head: 1\n",
            "Second Layer: False\n",
            "Score: 0.002595659167993636\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "hidden1: 352\n",
            "hidden2: 128\n",
            "attn: 0\n",
            "head: 0\n",
            "Second Layer: True\n",
            "Score: 0.002595840852027611\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "hidden1: 160\n",
            "hidden2: 192\n",
            "attn: 0\n",
            "head: 1\n",
            "Second Layer: True\n",
            "Score: 0.0025962796775505894\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "hidden1: 320\n",
            "hidden2: 192\n",
            "attn: 0\n",
            "head: 1\n",
            "Second Layer: False\n",
            "Score: 0.002596281621132105\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "hidden1: 32\n",
            "hidden2: 512\n",
            "attn: 0\n",
            "head: 0\n",
            "Second Layer: True\n",
            "Score: 0.0025963249390885705\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "hidden1: 64\n",
            "hidden2: 256\n",
            "attn: 0\n",
            "head: 1\n",
            "Second Layer: False\n",
            "Score: 0.002596362462149016\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "hidden1: 448\n",
            "hidden2: 320\n",
            "attn: 0\n",
            "head: 0\n",
            "Second Layer: True\n",
            "Score: 0.0025965765621217473\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "hidden1: 96\n",
            "hidden2: 416\n",
            "attn: 1\n",
            "head: 0\n",
            "Second Layer: False\n",
            "Score: 0.009882439622685077\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "hidden1: 192\n",
            "hidden2: 448\n",
            "attn: 1\n",
            "head: 0\n",
            "Second Layer: False\n",
            "Score: 0.011377635790360352\n"
          ]
        }
      ]
    }
  ]
}